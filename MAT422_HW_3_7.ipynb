{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeH3TZJiW2SbvGSlE04E7O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronbalanay/MAT-422/blob/main/MAT422_HW_3_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.7.1 Mathematical formulation\n",
        "\n",
        "We initialize weights and biases for each layer and propagate the input values through the network by first calculating z(l)=W(l)⋅a(l−1)+b(l) at each layer. After computing z(l), we apply the activation function σ(z(l)) to obtain a(l), the output for that layer. Finally, we update the activations from the previous layer to pass them to the next layer in sequence."
      ],
      "metadata": {
        "id": "rjMizU1mR2A3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V28cdJImRwer",
        "outputId": "30b95768-3c04-411b-80fb-2aa9f6eb6c2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1:\n",
            "z^1 =\n",
            "[[1.61275044]\n",
            " [2.42612712]\n",
            " [1.6062302 ]]\n",
            "a^1 =\n",
            "[[0.8337929 ]\n",
            " [0.91879805]\n",
            " [0.83288734]]\n",
            "\n",
            "Layer 2:\n",
            "z^2 =\n",
            "[[1.32817832]]\n",
            "a^2 =\n",
            "[[0.79053915]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the activation function (e.g., sigmoid) and its derivative\n",
        "def activation_function(z):\n",
        "    return 1 / (1 + np.exp(-z))  # Sigmoid activation\n",
        "\n",
        "# Define parameters for a simple neural network with two layers\n",
        "# Assume these are given or randomly initialized for a sample calculation\n",
        "layer_sizes = [2, 3, 1]  # Example sizes: 2 inputs, 3 nodes in hidden layer, 1 output\n",
        "np.random.seed(0)  # For reproducible results\n",
        "\n",
        "# Initialize weights and biases based on the layer sizes\n",
        "weights = [np.random.randn(layer_sizes[l], layer_sizes[l-1]) for l in range(1, len(layer_sizes))]\n",
        "biases = [np.random.randn(size, 1) for size in layer_sizes[1:]]\n",
        "\n",
        "# Sample input (column vector)\n",
        "a_prev = np.array([[0.5], [0.8]])  # Example input to the network\n",
        "\n",
        "# Forward pass through the layers based on the formulation\n",
        "for l, (W, b) in enumerate(zip(weights, biases), start=1):\n",
        "    # Compute z^(l) = W^(l) * a^(l-1) + b^(l)\n",
        "    z = np.dot(W, a_prev) + b\n",
        "    # Compute a^(l) = σ(z^(l))\n",
        "    a = activation_function(z)\n",
        "    # Print layer information\n",
        "    print(f\"Layer {l}:\")\n",
        "    print(f\"z^{l} =\\n{z}\")\n",
        "    print(f\"a^{l} =\\n{a}\\n\")\n",
        "    # Update a_prev for the next layer\n",
        "    a_prev = a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.7.2. Activation functions\n",
        "  \n",
        "Step Function:\n",
        "\n",
        "    Outputs 1 for values ≥ 0 and 0 otherwise.\n",
        "\n",
        "ReLU:\n",
        "\n",
        "    Outputs input values if they are positive, otherwise 0.\n",
        "\n",
        "Sigmoid:\n",
        "\n",
        "    Maps inputs to values between 0 and 1, useful for binary probabilities.\n",
        "\n",
        "Softmax:\n",
        "\n",
        "    Normalizes the input to a probability distribution, summing up to 1."
      ],
      "metadata": {
        "id": "WgWdWZCmR0Vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step Function\n",
        "def step_function(x):\n",
        "    return np.where(x >= 0, 1, 0)\n",
        "\n",
        "# ReLU (Rectified Linear Unit) Function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Sigmoid Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Softmax Function\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Subtracting max for numerical stability\n",
        "    return exp_x / exp_x.sum(axis=0)\n",
        "\n",
        "# Example inputs\n",
        "x = np.array([-1, 0, 1, 2])\n",
        "\n",
        "# Outputs\n",
        "print(\"Input:\", x)\n",
        "print(\"Step Function Output:\", step_function(x))\n",
        "print(\"ReLU Output:\", relu(x))\n",
        "print(\"Sigmoid Output:\", sigmoid(x))\n",
        "print(\"Softmax Output:\", softmax(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8a8cXrMTgG3",
        "outputId": "06106032-13a5-41fa-fbd3-34d49e1e59e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [-1  0  1  2]\n",
            "Step Function Output: [0 1 1 1]\n",
            "ReLU Output: [0 0 1 2]\n",
            "Sigmoid Output: [0.26894142 0.5        0.73105858 0.88079708]\n",
            "Softmax Output: [0.0320586  0.08714432 0.23688282 0.64391426]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.7.3. Cost function\n",
        "\n",
        "We define two functions,\n",
        "\n",
        "Mean Squared Error (MSE): Used for regression tasks, it calculates the average squared difference between the actual values (y_true) and predicted values (y_pred). The function returns a value that quantifies how well the model's predictions match the true values.\n",
        "\n",
        "Cross Entropy: Commonly used for binary classification, this function measures the difference between the true labels (y_true) and predicted probabilities (y_pred). It penalizes the model more heavily for incorrect predictions that are confident (i.e., closer to 0 or 1). This is especially useful when dealing with classification tasks that output probabilities."
      ],
      "metadata": {
        "id": "EIkPqW8TUgOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Mean Squared Error (MSE) for regression problems\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    N = y_true.shape[0]  # Number of data points\n",
        "    return 0.5 * np.sum((y_pred - y_true) ** 2) / N\n",
        "\n",
        "# Cross Entropy for binary classification\n",
        "def cross_entropy(y_true, y_pred):\n",
        "    # Clipping values to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "    return -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / y_true.shape[0]\n",
        "\n",
        "# Sample data for testing\n",
        "# For Mean Squared Error\n",
        "y_true_regression = np.array([3.0, 0.5, 2.1, 7.8])\n",
        "y_pred_regression = np.array([2.5, 0.7, 2.0, 7.5])\n",
        "\n",
        "# For Cross Entropy\n",
        "y_true_classification = np.array([1, 0, 1, 0])\n",
        "y_pred_classification = np.array([0.9, 0.1, 0.8, 0.3])\n",
        "\n",
        "# Calculating the cost for each\n",
        "mse_cost = mean_squared_error(y_true_regression, y_pred_regression)\n",
        "cross_entropy_cost = cross_entropy(y_true_classification, y_pred_classification)\n",
        "\n",
        "# Outputs\n",
        "print(\"Mean Squared Error Cost:\", mse_cost)\n",
        "print(\"Cross Entropy Cost:\", cross_entropy_cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvA4gsQ_VDn3",
        "outputId": "0a62f5f4-eec7-4648-ccbd-bef4427774e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error Cost: 0.04874999999999999\n",
            "Cross Entropy Cost: 0.19763488164214868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.7.4, 3.7.5 Backpropagation\n",
        "\n",
        "We implement a simple backpropagation algorithm with a two-layer neural network. The forward pass computes the activations of each layer, and the backward pass calculates the gradients (deltas) for each layer. The weights and biases are then updated using gradient descent based on these gradients. The sigmoid and relu activation functions are used, and their derivatives are computed during backpropagation. We iterate for multiple epochs, adjusting the weights and biases to minimize the cost (Mean Squared Error in this case)."
      ],
      "metadata": {
        "id": "RzZuKQa8VRAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_prime(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Cost function (Mean Squared Error for simplicity)\n",
        "def cost_function(y_true, y_pred):\n",
        "    return np.mean(0.5 * (y_true - y_pred) ** 2)\n",
        "\n",
        "# Backpropagation\n",
        "def backpropagate(X, y, weights, biases, activation_function, activation_prime, learning_rate=0.01):\n",
        "    # Forward pass\n",
        "    z = [X]\n",
        "    a = [X]\n",
        "    for l in range(len(weights)):\n",
        "        z_l = np.dot(a[-1], weights[l]) + biases[l]\n",
        "        a_l = activation_function(z_l)\n",
        "        z.append(z_l)\n",
        "        a.append(a_l)\n",
        "\n",
        "    # Compute cost (for monitoring)\n",
        "    cost = cost_function(y, a[-1])\n",
        "\n",
        "    # Backward pass\n",
        "    delta = [None] * len(weights)\n",
        "    delta[-1] = (a[-1] - y) * activation_prime(z[-1])  # output layer delta\n",
        "\n",
        "    # Backpropagate the deltas to earlier layers\n",
        "    for l in range(len(weights)-2, -1, -1):\n",
        "        delta[l] = np.dot(delta[l+1], weights[l+1].T) * activation_prime(z[l+1])\n",
        "\n",
        "    # Update weights and biases using gradient descent\n",
        "    for l in range(len(weights)):\n",
        "        weights[l] -= learning_rate * np.dot(a[l].T, delta[l])\n",
        "        biases[l] -= learning_rate * np.sum(delta[l], axis=0, keepdims=True)\n",
        "\n",
        "    return weights, biases, cost\n",
        "\n",
        "# Example usage\n",
        "np.random.seed(42)  # For reproducibility\n",
        "\n",
        "# Initialize data and network parameters\n",
        "X = np.random.randn(5, 3)  # 5 examples, 3 features\n",
        "y = np.random.randn(5, 1)  # 5 target values\n",
        "weights = [np.random.randn(3, 4), np.random.randn(4, 1)]  # Two layers\n",
        "biases = [np.random.randn(1, 4), np.random.randn(1, 1)]  # Bias for each layer\n",
        "\n",
        "# Train the network using backpropagation\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    weights, biases, cost = backpropagate(X, y, weights, biases, sigmoid, sigmoid_prime, learning_rate)\n",
        "\n",
        "    if epoch % 100 == 0:  # Print cost every 100 iterations\n",
        "        print(f\"Epoch {epoch}, Cost: {cost}\")\n",
        "\n",
        "# Final output after training\n",
        "print(\"Trained weights:\", weights)\n",
        "print(\"Trained biases:\", biases)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYysepBbVaWN",
        "outputId": "cc45ed7a-5b63-42d9-e5a7-7a7da3604b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Cost: 1.1420671896035153\n",
            "Epoch 100, Cost: 0.5599468233902358\n",
            "Epoch 200, Cost: 0.4812692778925115\n",
            "Epoch 300, Cost: 0.4596316116767897\n",
            "Epoch 400, Cost: 0.4499189320933734\n",
            "Epoch 500, Cost: 0.44446993634287174\n",
            "Epoch 600, Cost: 0.4410015084532769\n",
            "Epoch 700, Cost: 0.43860706760278256\n",
            "Epoch 800, Cost: 0.4368578835854288\n",
            "Epoch 900, Cost: 0.4355257442980521\n",
            "Trained weights: [array([[ 1.58675611,  0.03730541, -0.01691816, -1.17880037],\n",
            "       [-0.68099146, -0.3751891 , -1.07256865,  0.21584849],\n",
            "       [-0.6681502 , -0.69096226, -0.53547336,  1.78042101]]), array([[-2.03361315],\n",
            "       [-1.53020392],\n",
            "       [-0.26727963],\n",
            "       [-1.62900914]])]\n",
            "Trained biases: [array([[ 0.4158142 , -1.51982163, -1.43777548,  0.52909988]]), array([[-1.54702053]])]\n"
          ]
        }
      ]
    }
  ]
}
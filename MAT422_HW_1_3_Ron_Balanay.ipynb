{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO8TAZgS0Vc6sSei6fubZEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronbalanay/MAT-422/blob/main/MAT422_HW_1_3_Ron_Balanay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3.1 QR Decomposition\n",
        "\n",
        "Given a square matrix A with entries in **R**, we can decompose A as A = QR, where Q is an orthogonal matrix and R is an upper triangular matrix. We shall see an example below:\n",
        "\n"
      ],
      "metadata": {
        "id": "GCNzIBLylwid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "matrix_string = input(\"Please enter 4 numbers, separating by commas (i.e. 1,2,3,4): \")\n",
        "m = np.array([float(x) for x in matrix_string.split(',')])\n",
        "matrix = np.array([[m[0], m[1]], [m[2], m[3]]])\n",
        "A = np.array([[m[0], m[1]], [m[2], m[3]]])\n",
        "\n",
        "print(\"\\nLet A be a 2x2 matrix with entries in R:\")\n",
        "print(matrix)\n",
        "\n",
        "print(\"\\nWe can use Gram-Schmidt to orthogonalize A, yielding Q.\")\n",
        "\n",
        "a_1 = matrix[:, 0]\n",
        "a_2 = matrix[:, 1]\n",
        "\n",
        "print(\"\\nWe'll separate the columns of A as vectors: \")\n",
        "print(\"a_1 =\", a_1)\n",
        "print(\"a_2 =\", a_2)\n",
        "\n",
        "print(\"\\nNow, we'll use a_1 and a_2 to generate an orthonormal set q_1 and q_2, just as we did in chapter 1.2.3.\")\n",
        "print(\"First, we will normalize a_1 (call this q_1), and subtract the projection of a_2 onto q_1 from a_2 to generate q_2.\\n\")\n",
        "#normalize a_1, call this q_1\n",
        "a_1 /= np.linalg.norm(a_1)\n",
        "q_1 = a_1\n",
        "\n",
        "#subtract the projection of a_2 onto q_1 from a_2\n",
        "\n",
        "a_2 -= a_2.dot(q_1) * q_1\n",
        "\n",
        "#i had to store the value of a_2 at this point for calculating R later, without using a_2 as a reference variable\n",
        "w_2 = np.copy(a_2)\n",
        "\n",
        "a_2 /= np.linalg.norm(a_2)\n",
        "q_2 = a_2\n",
        "\n",
        "print(\"q_1 = \", q_1)\n",
        "print(\"q_2 = \", q_2)\n",
        "\n",
        "# verify that q_1, q_2 are orthogonal and account for floating point rounding error\n",
        "w_1 = q_1.dot(q_2)\n",
        "if(np.isclose(w_1,0)):\n",
        "  w_1 = np.zeros_like(q_1)\n",
        "\n",
        "Q = np.column_stack((q_1, q_2))\n",
        "\n",
        "print(\"\\nq_1 ⋅ q_2 =\", w_1)\n",
        "print(\"\\nQ =\\n\", Q)\n",
        "print(\"\\nNow, we need to find R. We can calculate R as R = (Q^T)(A) where Q^T is the transpose of Q.\")\n",
        "\n",
        "Q_T = Q.T\n",
        "\n",
        "print(\"\\nQ^T:\\n\", Q_T)\n",
        "\n",
        "R = np.zeros((2,2))\n",
        "R[0,0] = np.linalg.norm(A[:,0])\n",
        "R[1,1] = np.linalg.norm(w_2)\n",
        "R[0,1] = np.dot(Q[:, 0], A[:, 1])\n",
        "\n",
        "print(\"\\nR =\\n\", R)\n",
        "\n",
        "F = Q @ R\n",
        "\n",
        "print(\"\\nQ * R =\\n\", F)\n",
        "\n",
        "print(\"Now we have decomposed A = QR.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI-HC8NzpQpi",
        "outputId": "d9ae1914-5fa9-4cdc-e94c-ec0ba431f9e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter 4 numbers, separating by commas (i.e. 1,2,3,4): 9,2,5,3\n",
            "\n",
            "Let A be a 2x2 matrix with entries in R:\n",
            "[[9. 2.]\n",
            " [5. 3.]]\n",
            "\n",
            "We can use Gram-Schmidt to orthogonalize A, yielding Q.\n",
            "\n",
            "We'll separate the columns of A as vectors: \n",
            "a_1 = [9. 5.]\n",
            "a_2 = [2. 3.]\n",
            "\n",
            "Now, we'll use a_1 and a_2 to generate an orthonormal set q_1 and q_2, just as we did in chapter 1.2.3.\n",
            "First, we will normalize a_1 (call this q_1), and subtract the projection of a_2 onto q_1 from a_2 to generate q_2.\n",
            "\n",
            "q_1 =  [0.87415728 0.48564293]\n",
            "q_2 =  [-0.48564293  0.87415728]\n",
            "\n",
            "q_1 ⋅ q_2 = [0. 0.]\n",
            "\n",
            "Q =\n",
            " [[ 0.87415728 -0.48564293]\n",
            " [ 0.48564293  0.87415728]]\n",
            "\n",
            "Now, we need to find R. We can calculate R as R = (Q^T)(A) where Q^T is the transpose of Q.\n",
            "\n",
            "Q^T:\n",
            " [[ 0.87415728  0.48564293]\n",
            " [-0.48564293  0.87415728]]\n",
            "\n",
            "R =\n",
            " [[10.29563014  3.20524335]\n",
            " [ 0.          1.65118597]]\n",
            "\n",
            "Q * R =\n",
            " [[9. 2.]\n",
            " [5. 3.]]\n",
            "Now we have decomposed A = QR.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3.2 Least-Squares Approximation\n",
        "\n",
        "Suppose we have a system of equations Ax = b, where A is an m x n matrix, m > n, and b is in R^m. We would like to find an x in R^n such that this equation holds, but in general it is impossible as m > n. Therefore, we would like to find a solution that approximates Ax = b, and this solution should be minimized, i.e. r = b - Ax should be as close to zero as possible.\n",
        "\n",
        "In fact, we can use QR decomposition to our advantage. Given Ax = b, and A = QR, we have Ax = QRx, and some algebra yields x = (R^-1)b', where R^-1 is the inverse of R and b' = (Q^T)b.\n",
        "\n"
      ],
      "metadata": {
        "id": "FQjTbo-4R0Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "matrix_string = input(\"Please enter 6 numbers, separating by commas (i.e. 1,2,3,4,5,6): \")\n",
        "m = np.array([float(x) for x in matrix_string.split(',')])\n",
        "A = np.array([[m[0], m[1], m[2]], [m[3], m[4], m[5]]])\n",
        "\n",
        "print(\"\\nLet A be a 2 x 3 matrix with entries in R:\")\n",
        "print(A)\n",
        "\n",
        "#compute QR\n",
        "Q, R = np.linalg.qr(A)\n",
        "\n",
        "print(\"\\nBy the same method used in section 1.3.1, we decompose A into A = QR, where:\\n\")\n",
        "print(\"Q =\\n\", Q)\n",
        "print(\"R =\\n\", R)\n",
        "\n",
        "vector_string = input(\"\\nPlease enter 2 numbers, separating by commas (i.e. 1,2). These will be used for the vector b: \")\n",
        "b = np.array([float(x) for x in vector_string.split(',')])\n",
        "\n",
        "print(\"\\nLet b be a vector in R^2:\\n\", b)\n",
        "\n",
        "Q_T_b = Q.T @ b\n",
        "\n",
        "print(\"\\nQ^T * b =\\n\", Q_T_b)\n",
        "\n",
        "#Solving R * x = Q^T * b\n",
        "x, residuals, rank, s = np.linalg.lstsq(R, Q_T_b, rcond=None)\n",
        "\n",
        "print(\"\\nThe least squares solution x is:\\n\", x)\n",
        "\n",
        "w = A @ x\n",
        "print(\"\\nWe have A * x =\\n\", w)\n",
        "\n",
        "residual = A @ x - b\n",
        "\n",
        "print(\"\\nResidual of the solution:\\n\", residual)\n"
      ],
      "metadata": {
        "id": "qzBBqB2jUIEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09c7c4f-02c0-40bf-9e82-dd4c7f2f7eca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter 6 numbers, separating by commas (i.e. 1,2,3,4,5,6): 1,5,7,2,3,5\n",
            "\n",
            "Let A be a 2 x 3 matrix with entries in R:\n",
            "[[1. 5. 7.]\n",
            " [2. 3. 5.]]\n",
            "\n",
            "By the same method used in section 1.3.1, we decompose A into A = QR, where:\n",
            "\n",
            "Q =\n",
            " [[-0.4472136  -0.89442719]\n",
            " [-0.89442719  0.4472136 ]]\n",
            "R =\n",
            " [[-2.23606798 -4.91934955 -7.60263112]\n",
            " [ 0.         -3.13049517 -4.02492236]]\n",
            "\n",
            "Please enter 2 numbers, separating by commas (i.e. 1,2). These will be used for the vector b: 9,8\n",
            "\n",
            "Let b be a vector in R^2:\n",
            " [9. 8.]\n",
            "\n",
            "Q^T * b =\n",
            " [-11.18033989  -4.47213595]\n",
            "\n",
            "The least squares solution x is:\n",
            " [1.30136986 0.17808219 0.97260274]\n",
            "\n",
            "We have A * x =\n",
            " [9. 8.]\n",
            "\n",
            "Residual of the solution:\n",
            " [-1.77635684e-15  0.00000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3.3 Linear Regression\n",
        "\n",
        "Linear regression is a technique used to predict the value of unknown data by estimating the relationship between dependent and independent variables. Now, we can apply least-squares approximation to this problem.\n",
        "\n",
        "Let A be a matrix of input features, where the first column of A is reserved for intercepts. Let β be a vector of coefficients that includes the intercept (β0) and weights of each feature (β1, ... , βn) corresponding to the input feaatures in A. These weights will determine how much each feature influences the predicted outcome. Finally, let y be the vector of target values.\n",
        "\n",
        "(For example, suppose we are trying to predict house prices based on different input features such as size or location. The target values correspond to the price of each home in a dataset.)\n",
        "\n",
        "We can use least-squares to approximate the solution, β, to Aβ = y, which we will do below:\n"
      ],
      "metadata": {
        "id": "dTxL4J2RhEqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y_string = input(\"Please enter 3 target values (y), separating by commas (i.e. 1,2,3): \")\n",
        "y = np.array([float(x) for x in y_string.split(',')])\n",
        "\n",
        "print(\"\\nLet y be the vector of target values:\")\n",
        "print(\"y =\", y)\n",
        "\n",
        "feature_1 = input(\"\\nPlease enter the first feature using 3 data points, separating by commas (x11,x21,x31): \")\n",
        "feature_2 = input(\"Please enter the second feature using 3 data points, separating by commas (x12,x22,x32): \")\n",
        "\n",
        "x1 = np.array([float(x) for x in feature_1.split(',')])\n",
        "x2 = np.array([float(x) for x in feature_2.split(',')])\n",
        "\n",
        "print(\"\\nWe define our feature matrix X = [x1, x2]:\")\n",
        "print(\"x1 =\", x1)\n",
        "print(\"x2 =\", x2)\n",
        "\n",
        "#stack features horizontally\n",
        "\n",
        "X = np.column_stack((x1, x2))\n",
        "ones_column = np.ones((3, 1))\n",
        "\n",
        "#augment X\n",
        "A = np.hstack((ones_column, X))\n",
        "\n",
        "print(\"\\nWe augment the feature matrix X with a column of ones to account for the intercept term β0:\")\n",
        "print(\"A =\\n\", A)\n",
        "\n",
        "print(\"\\nNow, we'll use the least squares function to solve for β.\")\n",
        "\n",
        "beta, residuals, rank, s = np.linalg.lstsq(A, y, rcond=None)\n",
        "\n",
        "#intercept, weights\n",
        "beta_0 = beta[0]\n",
        "beta_1, beta_2 = beta[1], beta[2]\n",
        "\n",
        "print(\"\\nThe least-squares solution β is:\\n\", beta)\n",
        "print(\"\\nWhere\")\n",
        "print(f\"β0 (intercept) = {beta_0}\")\n",
        "print(f\"β1 (weight of feature 1) = {beta_1}\")\n",
        "print(f\"β2 (weight of feature 2) = {beta_2}\")\n",
        "\n",
        "y_pred = A @ beta\n",
        "\n",
        "print(\"\\nOur predicted values are ŷ =\\n\", y_pred)\n",
        "\n",
        "print(\"\\nOur residual values are y - ŷ =\\n\", y - y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8esYERmK4It",
        "outputId": "fe200e36-9a1f-47aa-ddc3-90445065a0d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter 3 target values (y), separating by commas (i.e. 1,2,3): 1,2,4\n",
            "\n",
            "Let y be the vector of target values:\n",
            "y = [1. 2. 4.]\n",
            "\n",
            "Please enter the first feature using 3 data points, separating by commas (x11,x21,x31): 8,7,6\n",
            "Please enter the second feature using 3 data points, separating by commas (x12,x22,x32): 5,9,1\n",
            "\n",
            "We define our feature matrix X = [x1, x2]:\n",
            "x1 = [8. 7. 6.]\n",
            "x2 = [5. 9. 1.]\n",
            "\n",
            "We augment the feature matrix X with a column of ones to account for the intercept term β0:\n",
            "A =\n",
            " [[1. 8. 5.]\n",
            " [1. 7. 9.]\n",
            " [1. 6. 1.]]\n",
            "\n",
            "Now, we'll use the least squares function to solve for β.\n",
            "\n",
            "The least-squares solution β is:\n",
            " [12.08333333 -1.33333333 -0.08333333]\n",
            "\n",
            "Where\n",
            "β0 (intercept) = 12.083333333333334\n",
            "β1 (weight of feature 1) = -1.3333333333333328\n",
            "β2 (weight of feature 2) = -0.08333333333333359\n",
            "\n",
            "Our predicted values are ŷ =\n",
            " [1. 2. 4.]\n",
            "\n",
            "Our residual values are y - ŷ =\n",
            " [-3.55271368e-15 -1.33226763e-15 -3.55271368e-15]\n"
          ]
        }
      ]
    }
  ]
}
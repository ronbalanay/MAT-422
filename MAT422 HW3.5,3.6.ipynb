{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI2yV1oypDN5cf8/F+T1WW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronbalanay/MAT-422/blob/main/MAT422%20HW3.5%2C3.6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.5 K-Means\n",
        "\n",
        "We implement the k-means clustering algorithm to partition a set of observations into *k* clusters, with each observation assigned to the nearest cluster center. This algorithm minimizes the within-cluster sum of squares (WCSS) to ensure observations in each cluster are closely grouped around their centroid. We start by selecting kk random points as initial centroids, then assign each observation to its nearest centroid using Euclidean distance. Next, we recompute the centroid of each cluster by calculating the mean of points within it. These steps of reassignment and recomputation are repeated until convergence, when no observations switch clusters between iterations. Our function outputs the final cluster assignments, centroids, and the minimized WCSS values, demonstrating the algorithm's effectiveness in achieving local optimality for clustering.\n",
        "\n",
        "This code partitions the data into clusters based on proximity to centroids, iteratively updating until convergence. The function returns and prints the data, cluster assignments, final centroids, and minimized WCSS. This demonstrates k-means clusteringâ€™s capacity to group data points by minimizing distances to cluster centers."
      ],
      "metadata": {
        "id": "6VMfOJOlbsBP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td7uBBoeUR66",
        "outputId": "42477621-cbf9-409b-f24a-91d1e3f11ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: [[0.17815032 0.16649381]\n",
            " [0.26395108 0.09592343]\n",
            " [0.0489664  0.70168597]\n",
            " [0.39328807 0.19526838]\n",
            " [0.0786828  0.45932218]\n",
            " [0.46469784 0.69805083]\n",
            " [0.70609367 0.45976031]\n",
            " [0.76773662 0.73898506]\n",
            " [0.83185063 0.33068694]\n",
            " [0.36928968 0.04275911]\n",
            " [0.13435237 0.77124807]\n",
            " [0.87680827 0.56379172]\n",
            " [0.74995786 0.34686866]\n",
            " [0.12162982 0.52562682]\n",
            " [0.68830562 0.7706164 ]\n",
            " [0.35481756 0.96936797]\n",
            " [0.76970986 0.13308975]\n",
            " [0.68582971 0.2483069 ]\n",
            " [0.55575035 0.43682401]\n",
            " [0.99281436 0.83198675]\n",
            " [0.90062359 0.02362868]\n",
            " [0.57813343 0.71573626]\n",
            " [0.94034646 0.44281004]\n",
            " [0.30550552 0.0907058 ]\n",
            " [0.96180695 0.42356202]\n",
            " [0.52879098 0.18129932]\n",
            " [0.18197054 0.61245402]\n",
            " [0.98562424 0.32168385]\n",
            " [0.56411705 0.08023373]\n",
            " [0.57295438 0.43803939]\n",
            " [0.71552084 0.27692609]\n",
            " [0.65778979 0.98413155]\n",
            " [0.25083718 0.83160855]\n",
            " [0.32760401 0.06460335]\n",
            " [0.0781543  0.65326539]\n",
            " [0.36837947 0.23884985]\n",
            " [0.45488507 0.16857916]\n",
            " [0.7134972  0.54057113]\n",
            " [0.93834854 0.76852562]\n",
            " [0.93260223 0.42240197]\n",
            " [0.87460579 0.84124623]\n",
            " [0.70134402 0.28106309]\n",
            " [0.57255824 0.44924456]\n",
            " [0.70786197 0.51657411]\n",
            " [0.53023829 0.48430101]\n",
            " [0.51888246 0.99025031]\n",
            " [0.21598695 0.30989865]\n",
            " [0.19966579 0.47453507]\n",
            " [0.07576113 0.12350089]\n",
            " [0.07719363 0.87882631]\n",
            " [0.60244735 0.47318598]\n",
            " [0.80043827 0.27837366]\n",
            " [0.88434313 0.30128568]\n",
            " [0.56301785 0.41198343]\n",
            " [0.86893301 0.15022915]\n",
            " [0.44802204 0.15618295]\n",
            " [0.38798989 0.63578067]\n",
            " [0.18529255 0.11238413]\n",
            " [0.58648534 0.67400076]\n",
            " [0.17803872 0.88939011]\n",
            " [0.14426658 0.55031112]\n",
            " [0.26258961 0.3629849 ]\n",
            " [0.7241599  0.40427077]\n",
            " [0.1678821  0.5491462 ]\n",
            " [0.75425829 0.04746165]\n",
            " [0.37002401 0.51495644]\n",
            " [0.13774893 0.03340793]\n",
            " [0.1413617  0.18666786]\n",
            " [0.79364325 0.75316932]\n",
            " [0.97060792 0.94796287]\n",
            " [0.19578451 0.38180162]\n",
            " [0.12487403 0.08917503]\n",
            " [0.65635565 0.19590164]\n",
            " [0.05411945 0.33486001]\n",
            " [0.17102459 0.97718846]\n",
            " [0.86593376 0.71816728]\n",
            " [0.38424186 0.59798161]\n",
            " [0.16414505 0.63508853]\n",
            " [0.27104634 0.96453326]\n",
            " [0.5745798  0.03844078]\n",
            " [0.30051749 0.56918174]\n",
            " [0.18384913 0.23933252]\n",
            " [0.72637983 0.96117901]\n",
            " [0.24474199 0.25263391]\n",
            " [0.92978251 0.13492684]\n",
            " [0.97842975 0.35377377]\n",
            " [0.34877198 0.06168844]\n",
            " [0.93212608 0.16927043]\n",
            " [0.54880993 0.65265213]\n",
            " [0.4981587  0.90171129]\n",
            " [0.31896351 0.5067405 ]\n",
            " [0.99139114 0.59515382]\n",
            " [0.51924006 0.29215822]\n",
            " [0.39610868 0.90509702]\n",
            " [0.90113218 0.05173261]\n",
            " [0.84163502 0.65976675]\n",
            " [0.64034253 0.93201523]\n",
            " [0.10679974 0.17564176]\n",
            " [0.34358681 0.29837346]\n",
            " [0.66554304 0.77884199]]\n",
            "Cluster Assignments: [2 2 2 2 2 0 1 0 1 2 0 1 1 2 0 0 1 1 1 0 1 0 1 2 1 1 2 1 1 1 1 0 0 2 2 2 2\n",
            " 1 0 1 0 1 1 1 1 0 2 2 2 0 1 1 1 1 1 2 0 2 0 0 2 2 1 2 1 2 2 2 0 0 2 2 1 2\n",
            " 0 0 0 2 0 1 2 2 0 2 1 1 2 1 0 0 2 1 1 0 1 0 0 2 2 0]\n",
            "Centroids: [[0.55946568 0.8186213 ]\n",
            " [0.7551256  0.31388377]\n",
            " [0.2258632  0.31240088]]\n",
            "Final WCSS: 6.056855454334686\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2, 2, 2, 2, 2, 0, 1, 0, 1, 2, 0, 1, 1, 2, 0, 0, 1, 1, 1, 0, 1, 0,\n",
              "        1, 2, 1, 1, 2, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2, 1, 0, 1, 0, 1, 1, 1,\n",
              "        1, 0, 2, 2, 2, 0, 1, 1, 1, 1, 1, 2, 0, 2, 0, 0, 2, 2, 1, 2, 1, 2,\n",
              "        2, 2, 0, 0, 2, 2, 1, 2, 0, 0, 0, 2, 0, 1, 2, 2, 0, 2, 1, 1, 2, 1,\n",
              "        0, 0, 2, 1, 1, 0, 1, 0, 0, 2, 2, 0]),\n",
              " array([[0.55946568, 0.8186213 ],\n",
              "        [0.7551256 , 0.31388377],\n",
              "        [0.2258632 , 0.31240088]]),\n",
              " 6.056855454334686)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def kmeans(data, k, max_iters=100, tolerance=1e-4):\n",
        "    # randomly initialize k centroids from data points\n",
        "    centroids = data[np.random.choice(data.shape[0], k, replace=False)]\n",
        "\n",
        "    for iteration in range(max_iters):\n",
        "        # assign each point to the nearest centroid\n",
        "        distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n",
        "        clusters = np.argmin(distances, axis=1)\n",
        "\n",
        "        # calculate new centroids by averaging points in each cluster\n",
        "        new_centroids = np.array([data[clusters == i].mean(axis=0) for i in range(k)])\n",
        "\n",
        "        # check for convergence by evaluating centroid change\n",
        "        if np.all(np.abs(new_centroids - centroids) < tolerance):\n",
        "            break\n",
        "        centroids = new_centroids\n",
        "\n",
        "    # calculate final within-cluster sum of squares (WCSS)\n",
        "    wcss = sum(np.sum((data[clusters == i] - centroids[i]) ** 2) for i in range(k))\n",
        "\n",
        "    print(\"Data:\", data)\n",
        "    print(\"Cluster Assignments:\", clusters)\n",
        "    print(\"Centroids:\", centroids)\n",
        "    print(\"Final WCSS:\", wcss)\n",
        "    return clusters, centroids, wcss\n",
        "\n",
        "# example usage with random data\n",
        "data = np.random.rand(100, 2)\n",
        "kmeans(data, k=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.6 Support Vector Machine\n",
        "We implement a basic Support Vector Machine (SVM) for binary classification using stochastic gradient descent (SGD). The SVM model aims to find an optimal hyperplane that separates two classes with the maximum possible margin. We begin by initializing the SVM with a learning rate, a regularization parameter (lambda) to control the trade-off between maximizing margin size and correctly classifying data points, and the number of iterations for training. The fit function iteratively updates the weight vector and bias term based on whether each data point lies on the correct side of the margin boundary. For each training point, if it satisfies the condition yi(wâ‹…xiâˆ’b)â‰¥1 (meaning itâ€™s correctly classified), only the regularization term influences the weight update. If misclassified, however, both the regularization term and the data pointâ€™s contribution adjust the weights and bias to help align the point with the desired class boundary. Finally, in the predict function, the model computes the sign of each test pointâ€™s distance from the hyperplane to classify it. This approach can be easily extended to more complex datasets, where tuning the learning rate, lambda, and iterations helps the model generalize and reach an optimal separation boundary."
      ],
      "metadata": {
        "id": "0ApOU84lfWwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SupportVectorMachine:\n",
        "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "        # initializing learning rate, lambda for regularization, and number of iterations\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda_param = lambda_param\n",
        "        self.n_iters = n_iters\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # initializing weights and bias, number of samples, and features\n",
        "        n_samples, n_features = X.shape\n",
        "        self.w = np.zeros(n_features)\n",
        "        self.b = 0\n",
        "\n",
        "        # gradient descent optimization\n",
        "        for _ in range(self.n_iters):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                condition = y[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
        "                if condition:\n",
        "                    # update weights and bias for correctly classified samples\n",
        "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    # update weights and bias for misclassified samples\n",
        "                    self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y[idx]))\n",
        "                    self.b -= self.learning_rate * y[idx]\n",
        "\n",
        "    def predict(self, X):\n",
        "        # predict the class by finding the sign of the distance from the hyperplane\n",
        "        linear_output = np.dot(X, self.w) - self.b\n",
        "        return np.sign(linear_output)\n",
        "\n",
        "\n",
        "# example usage:\n",
        "# let's create some data and train an SVM model with it\n",
        "if __name__ == \"__main__\":\n",
        "    # creating a dataset\n",
        "    X = np.array([\n",
        "        [1, 2],\n",
        "        [2, 3],\n",
        "        [3, 3],\n",
        "        [2, 1],\n",
        "        [3, 2]\n",
        "    ])\n",
        "    y = np.array([1, 1, 1, -1, -1])  # labels must be either 1 or -1\n",
        "\n",
        "    # initializing and training the SVM\n",
        "    svm = SupportVectorMachine(learning_rate=0.001, lambda_param=0.01, n_iters=1000)\n",
        "    svm.fit(X, y)\n",
        "\n",
        "    # making predictions\n",
        "    predictions = svm.predict(X)\n",
        "    print(\"Predictions:\", predictions)\n",
        "\n",
        "    # printing the model parameters\n",
        "    print(\"Weights:\", svm.w)\n",
        "    print(\"Bias:\", svm.b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORA3tZzzcuS4",
        "outputId": "06888e05-3621-4884-cdd3-8271f7889a38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [ 1.  1.  1. -1. -1.]\n",
            "Weights: [-1.01430102  1.25080626]\n",
            "Bias: 0.23700000000000018\n"
          ]
        }
      ]
    }
  ]
}
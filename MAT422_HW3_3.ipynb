{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+uDn7k8qhQQll7zm8PMaI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronbalanay/MAT-422/blob/main/MAT422_HW3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3.1. Necessary and sufficient conditions of local minimizers\n",
        "\n",
        "We check whether a point is a global and local minimizer for the function f(x)=∑x_i^2 using the gradient and Hessian. We verifies the first-order necessary condition (gradient is zero) and the second-order sufficient condition (Hessian is positive definite). The function is printed, and tests are run to identify if the point is a minimizer.\n"
      ],
      "metadata": {
        "id": "YFtGPRf4zi8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from numpy.linalg import eigvals\n",
        "\n",
        "# define the function f(x)\n",
        "def f(x):\n",
        "    return np.sum(x**2)  # example function: f(x) = sum(x_i^2), which is convex\n",
        "\n",
        "# gradient of the function f(x)\n",
        "def grad_f(x):\n",
        "    return 2 * x  # gradient: ∇f(x) = 2*x\n",
        "\n",
        "# hessian of the function f(x)\n",
        "def hessian_f(x):\n",
        "    return 2 * np.eye(len(x))  # hessian: 2*identity matrix (positive definite)\n",
        "\n",
        "# check for global minimizer\n",
        "def is_global_minimizer(x_star):\n",
        "    # a convex function like sum(x_i^2) has a global minimum at x = 0\n",
        "    f_x_star = f(x_star)\n",
        "    for _ in range(1000):\n",
        "        x_rand = np.random.uniform(-10, 10, len(x_star))\n",
        "        if f(x_rand) < f_x_star:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# check for local minimizer using first-order necessary condition\n",
        "def is_local_minimizer(x_star):\n",
        "    grad = grad_f(x_star)\n",
        "    if np.allclose(grad, np.zeros_like(grad), atol=1e-5):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# second-order sufficient condition (check positive definiteness of hessian)\n",
        "def second_order_sufficient_condition(x_star):\n",
        "    hessian = hessian_f(x_star)\n",
        "    eigenvalues = eigvals(hessian)\n",
        "    if np.all(eigenvalues > 0):  # positive definite hessian\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# print the function being referenced\n",
        "def print_function():\n",
        "    print(\"Function f(x): sum(x_i^2)\")\n",
        "\n",
        "# test point for minimization\n",
        "x_star = np.array([0.0, 0.0])\n",
        "\n",
        "# print the function we are referencing\n",
        "print_function()\n",
        "\n",
        "# global minimizer check\n",
        "if is_global_minimizer(x_star):\n",
        "    print(f\"{x_star} is a global minimizer.\")\n",
        "else:\n",
        "    print(f\"{x_star} is NOT a global minimizer.\")\n",
        "\n",
        "# local minimizer check\n",
        "if is_local_minimizer(x_star):\n",
        "    print(f\"{x_star} satisfies the first-order necessary condition for a local minimizer.\")\n",
        "else:\n",
        "    print(f\"{x_star} does NOT satisfy the first-order necessary condition for a local minimizer.\")\n",
        "\n",
        "# second-order sufficient condition check\n",
        "if second_order_sufficient_condition(x_star):\n",
        "    print(f\"{x_star} satisfies the second-order sufficient condition and is a strict local minimizer.\")\n",
        "else:\n",
        "    print(f\"{x_star} does NOT satisfy the second-order sufficient condition.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOBiT4AmzlGl",
        "outputId": "9af838e0-ecc0-42dd-eff2-11a61d809bcb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function f(x): sum(x_i^2)\n",
            "[0. 0.] is a global minimizer.\n",
            "[0. 0.] satisfies the first-order necessary condition for a local minimizer.\n",
            "[0. 0.] satisfies the second-order sufficient condition and is a strict local minimizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3.2. Convexity and global minimizers\n",
        "We check whether a set of points forms a convex set and whether an open ball in R^2 is convex. We verify the convexity by checking if any convex combination of two points in the set or ball still lies within the set or ball. The set of points is printed, and tests are run to determine if the set and the open ball are convex."
      ],
      "metadata": {
        "id": "E4k3np2Szlkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# function to check if a set D is convex\n",
        "def is_convex_set(D):\n",
        "    \"\"\"\n",
        "    Check if the set D ⊆ R^d is convex.\n",
        "\n",
        "    Arguments:\n",
        "    D : list of numpy arrays - Points that belong to set D.\n",
        "\n",
        "    Returns:\n",
        "    bool : True if D is convex, False otherwise.\n",
        "    \"\"\"\n",
        "    for i, x in enumerate(D):\n",
        "        for j, y in enumerate(D):\n",
        "            if i != j:\n",
        "                # check for all alpha in [0, 1]\n",
        "                alpha_values = np.linspace(0, 1, 100)\n",
        "                for alpha in alpha_values:\n",
        "                    z = (1 - alpha) * x + alpha * y\n",
        "                    if not any(np.allclose(z, point) for point in D):\n",
        "                        return False\n",
        "    return True\n",
        "\n",
        "# example: Open ball in R^d is convex\n",
        "def open_ball_is_convex(x0, delta, points):\n",
        "    \"\"\"\n",
        "    Check if an open ball B_delta(x0) is convex.\n",
        "\n",
        "    Arguments:\n",
        "    x0 : numpy array - Center of the open ball.\n",
        "    delta : float - Radius of the open ball.\n",
        "    points : list of numpy arrays - Points that belong to the open ball.\n",
        "\n",
        "    Returns:\n",
        "    bool : True if B_delta(x0) is convex, False otherwise.\n",
        "    \"\"\"\n",
        "    for x in points:\n",
        "        for y in points:\n",
        "            for alpha in np.linspace(0, 1, 100):\n",
        "                w = (1 - alpha) * x + alpha * y\n",
        "                if np.linalg.norm(w - x0) > delta:\n",
        "                    return False\n",
        "    return True\n",
        "\n",
        "# test case: Points in a convex set (e.g., unit ball in R^2)\n",
        "x0 = np.array([0, 0])\n",
        "delta = 1\n",
        "points_in_ball = [np.array([0.5, 0.5]), np.array([0.3, -0.7]), np.array([-0.6, 0.6])]\n",
        "\n",
        "# print the set we are working with\n",
        "print(\"Points in the set we are working with:\", points_in_ball)\n",
        "\n",
        "# check if the points form a convex set and if the open ball is convex\n",
        "is_convex = is_convex_set(points_in_ball)\n",
        "is_ball_convex = open_ball_is_convex(x0, delta, points_in_ball)\n",
        "\n",
        "print(\"Is the set convex?\", is_convex)\n",
        "print(\"Is the open ball convex?\", is_ball_convex)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C67WoLdsznbf",
        "outputId": "cbec475c-55dd-42c1-bc6d-ca50a52b1813"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Points in the set we are working with: [array([0.5, 0.5]), array([ 0.3, -0.7]), array([-0.6,  0.6])]\n",
            "Is the set convex? False\n",
            "Is the open ball convex? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3.3. Gradient descent\n",
        "\n",
        "We use the gradient descent algorithm to find the local minimum of the function f(x)=x^TAx−b^Tx, where A is a matrix and b is a vector. We start at an initial point and iteratively update the point in the direction of the negative gradient. The gradient is computed at each iteration using the gradient_f function, and the point is updated based on the learning rate. The process continues until the gradient is sufficiently small or the maximum number of iterations is reached. The final output provides the coordinates of the global minimizer and the function's value at that point."
      ],
      "metadata": {
        "id": "cOE3PjO3zpoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# define the function f(x) for minimization\n",
        "def f(x):\n",
        "    A = np.array([[3, 2], [2, 6]])  # example matrix A\n",
        "    b = np.array([2, -8])            # example vector b\n",
        "    return np.dot(x.T, np.dot(A, x)) - np.dot(b.T, x)\n",
        "\n",
        "# compute the gradient of f(x)\n",
        "def gradient_f(x):\n",
        "    A = np.array([[3, 2], [2, 6]])  # example matrix A\n",
        "    b = np.array([2, -8])            # example vector b\n",
        "    return np.dot(A, x) - b\n",
        "\n",
        "# gradient descent algorithm\n",
        "def gradient_descent(starting_point, learning_rate, tolerance, max_iterations):\n",
        "    x_k = starting_point\n",
        "    for k in range(max_iterations):\n",
        "        grad = gradient_f(x_k)  # calculate the gradient at the current point\n",
        "        if np.linalg.norm(grad) < tolerance:  # check if the gradient is close to zero\n",
        "            break\n",
        "        x_k = x_k - learning_rate * grad  # update the point\n",
        "    return x_k\n",
        "\n",
        "# parameters for gradient descent\n",
        "starting_point = np.array([0.0, 0.0])  # initial guess\n",
        "learning_rate = 0.1                    # step size\n",
        "tolerance = 1e-6                        # convergence tolerance\n",
        "max_iterations = 100                    # maximum number of iterations\n",
        "\n",
        "# execute gradient descent\n",
        "minimizer = gradient_descent(starting_point, learning_rate, tolerance, max_iterations)\n",
        "\n",
        "# output the results\n",
        "print(\"Function value at minimizer:\", f(minimizer))\n",
        "print(\"Global minimizer:\", minimizer)\n",
        "print(\"Matrix A:\\n\", np.array([[3, 2], [2, 6]]))\n",
        "print(\"Vector b:\", np.array([2, -8]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhZaazSQzrW2",
        "outputId": "c49001fc-65f2-405a-fa9f-ef4bb6395ca9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function value at minimizer: -2.3695261717193716e-06\n",
            "Global minimizer: [ 1.99999961 -1.9999998 ]\n",
            "Matrix A:\n",
            " [[3 2]\n",
            " [2 6]]\n",
            "Vector b: [ 2 -8]\n"
          ]
        }
      ]
    }
  ]
}
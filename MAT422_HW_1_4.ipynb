{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMdZL270dqvuPYnC7nVb6mV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronbalanay/MAT-422/blob/main/MAT422_HW_1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4.1 Singular Value Decomposition\n",
        "\n",
        "Let A be any m by n matrix. Then there exist matrices U, S, and V (that are m \\* m, m \\* n, and n \\* n, resp.) such that A = U \\* S \\* V^T, where V^T is the transpose of V (also note that U, V are orthogonal). This technique, singular value decomposition (SVD), is especially powerful when you want to reduce the size of a matrix and eliminate less important data, such as in image compression or speeding up computation of large data."
      ],
      "metadata": {
        "id": "cxJqRdKTzQji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "matrix_string = input(\"Please enter 4 numbers, separating by commas (i.e. 1,2,3,4): \")\n",
        "m = np.array([float(x) for x in matrix_string.split(',')])\n",
        "matrix = np.array([[m[0], m[1]], [m[2], m[3]]])\n",
        "A = np.array([[m[0], m[1]], [m[2], m[3]]])\n",
        "\n",
        "print(\"\\nLet A be a 2x2 matrix with entries in R:\")\n",
        "print(matrix)\n",
        "\n",
        "#i tried to do this manually at first by following the process described,\n",
        "#but i ran into discrepancies with the ordering of eigenvalues and eigenvectors using np.eig as opposed to using .svd\n",
        "\n",
        "U, S, V_T = np.linalg.svd(A)\n",
        "\n",
        "print(\"\\nNow, we need to find the eigenvectors of A * A^T, and we let these eigenvectors be the columns of U:\\n\", U)\n",
        "print(\"\\nWe will repeat this process with A^T * A, finding a matrix V, and take its transpose V^T:\\n\", V_T)\n",
        "print(\"\\nFinally, we can take the square roots of the eigenvalues corresponding to U or V as a diagonal matrix S:\\n\", np.diag(S))\n",
        "\n",
        "S_matrix = np.zeros_like(A)\n",
        "np.fill_diagonal(S_matrix, S)\n",
        "A_svd = U @ S_matrix @ V_T\n",
        "\n",
        "print(\"\\nWe see that U * S * V^T = \\n\", A_svd)\n",
        "\n",
        "if np.allclose(A, A_svd):\n",
        "    print(\"\\nIndeed, A = U *S * V^T.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dqDOnWp_KoJ",
        "outputId": "34093de0-a67e-4cd6-f9c0-dcfc7b6abbb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter 4 numbers, separating by commas (i.e. 1,2,3,4): 9,2,6,7\n",
            "\n",
            "Let A be a 2x2 matrix with entries in R:\n",
            "[[9. 2.]\n",
            " [6. 7.]]\n",
            "\n",
            "Now, we need to find the eigenvectors of A * A^T, and we let these eigenvectors be the columns of U:\n",
            " [[-0.70710678 -0.70710678]\n",
            " [-0.70710678  0.70710678]]\n",
            "\n",
            "We will repeat this process with A^T * A, finding a matrix V, and take its transpose V^T:\n",
            " [[-0.85749293 -0.51449576]\n",
            " [-0.51449576  0.85749293]]\n",
            "\n",
            "Finally, we can take the square roots of the eigenvalues corresponding to U or V as a diagonal matrix S:\n",
            " [[12.36931688  0.        ]\n",
            " [ 0.          4.12310563]]\n",
            "\n",
            "We see that U * S * V^T = \n",
            " [[9. 2.]\n",
            " [6. 7.]]\n",
            "\n",
            "Indeed, A = U *S * V^T.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4.2 Low-Rank Matrix Approximations\n",
        "\n",
        "As mentioned in the previous section, we can use SVD to produce a lossy compression of a matrix, namely by reducing the rank of said matrix. Suppose A is m by n. In order to describe A, we need m\\*n numbers, while a rank-k approximation of A only requires k(m + n) numbers. Therefore, a rank-k approximation becomes more useful as the size of A increases."
      ],
      "metadata": {
        "id": "E3XTh8esD_kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "matrix_string = input(\"Please enter 4 numbers, separating by commas (i.e. 1,2,3,4): \")\n",
        "m = np.array([float(x) for x in matrix_string.split(',')])\n",
        "matrix = np.array([[m[0], m[1]], [m[2], m[3]]])\n",
        "A = np.array([[m[0], m[1]], [m[2], m[3]]])\n",
        "\n",
        "print(\"\\nLet A be a 2x2 matrix with entries in R:\")\n",
        "print(matrix)\n",
        "\n",
        "U, S, V_T = np.linalg.svd(A)\n",
        "print(\"\\nAgain, we use singular value decomposition to produce matrices U, S, and V^T such that A = U * S * V^T.\")\n",
        "print(\"\\nU:\\n\", U)\n",
        "print(\"\\nS:\\n\", S)\n",
        "print(\"\\nV^T:\\n\", V_T)\n",
        "print(\"\\nIn order to find a rank-k approximation of A, we take the sum from i=1 to k of s_i(u_i * v_i^T),\\nwhere s_i are the singular values of S, and u_i, v_i^T are the singular vectors of U and V^T.\")\n",
        "k1 = 1\n",
        "U_k1 = U[:, :k1]\n",
        "S_k1 = np.diag(S[:k1])\n",
        "V_T_k1 = V_T[:k1, :]\n",
        "A_approx_k1 = U_k1 @ S_k1 @ V_T_k1\n",
        "\n",
        "print(\"\\nA rank-1 approximation of A only uses the largest singular value from S:\\n\", A_approx_k1)\n",
        "\n",
        "\n",
        "k2 = 2\n",
        "U_k2 = U[:, :k2]\n",
        "S_k2 = np.diag(S[:k2])\n",
        "V_T_k2 = V_T[:k2, :]\n",
        "A_approx_k2 = U_k2 @ S_k2 @ V_T_k2\n",
        "\n",
        "print(\"\\nOn the other hand, the rank-2 approximation uses both singular values from S:\\n\", A_approx_k2)\n",
        "print(\"\\nSince our rank-2 approximation is the same rank as A, it is precisely the same as A.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62D-UaChsYTJ",
        "outputId": "e9ebf341-036c-4bf3-fa2b-11f00f93fce8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter 4 numbers, separating by commas (i.e. 1,2,3,4): 98,2,389,8\n",
            "\n",
            "Let A be a 2x2 matrix with entries in R:\n",
            "[[ 98.   2.]\n",
            " [389.   8.]]\n",
            "\n",
            "Again, we use singular value decomposition to produce matrices U, S, and V^T such that A = U * S * V^T.\n",
            "\n",
            "U:\n",
            " [[-0.24429411 -0.96970118]\n",
            " [-0.96970118  0.24429411]]\n",
            "\n",
            "S:\n",
            " [4.01239330e+02 1.49536687e-02]\n",
            "\n",
            "V^T:\n",
            " [[-0.99978879 -0.02055182]\n",
            " [-0.02055182  0.99978879]]\n",
            "\n",
            "In order to find a rank-k approximation of A, we take the sum from i=1 to k of s_i(u_i * v_i^T),\n",
            "where s_i are the singular values of S, and u_i, v_i^T are the singular vectors of U and V^T.\n",
            "\n",
            "A rank-1 approximation of A only uses the largest singular value from S:\n",
            " [[ 97.99970199   2.01449753]\n",
            " [389.00007508   7.99634768]]\n",
            "\n",
            "On the other hand, the rank-2 approximation uses both singular values from S:\n",
            " [[ 98.   2.]\n",
            " [389.   8.]]\n",
            "\n",
            "Since our rank-2 approximation is the same rank as A, it is precisely the same as A.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4.3 Principal Component Analysis\n",
        "\n",
        "Suppose we have a high-dimensional dataset with several different components. There may be a lot of variation in our data, and we would like to understand which components are causing this variation. Since there are so many components, this is a difficult task- but principal component analysis (PCA) allows us to do just that. We can reduce the dimension of our data by projecting each vector onto the components that have the most influence on our data's variation.\n",
        "\n",
        "In fact, principal component analysis is a special case of singular value decomposition, so we will use SVD in the below example."
      ],
      "metadata": {
        "id": "NQyqtd70HbcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "matrix_string = input(\"Please enter 6 numbers, separating by commas (i.e. 1,2,3,4,5,6): \")\n",
        "m = np.array([float(x) for x in matrix_string.split(',')])\n",
        "A = np.array([[m[0], m[1], m[2]],\n",
        "              [m[3], m[4], m[5]]])\n",
        "\n",
        "print(\"\\nLet A be a 2x3 matrix with entries in R:\")\n",
        "print(A)\n",
        "\n",
        "A_mean = A - np.mean(A, axis=0)\n",
        "print(\"\\nFirst, we need to center the data by subtracting the mean of each column.\")\n",
        "print(\"This will simplify the calculations we have to do later and remove bias from columns with large magnitudes:\\n\", A_mean)\n",
        "\n",
        "U, S, V_T = np.linalg.svd(A_mean)\n",
        "print(\"\\nWe'll use singular value decomposition on A to find matrices U, S, and V^T.\")\n",
        "print(\"\\nU:\\n\", U)\n",
        "print(\"\\nS (Singular values):\\n\", S)\n",
        "print(\"\\nV^T (our principal components):\\n\", V_T)\n",
        "\n",
        "V_T_1 = V_T[:1, :]\n",
        "A_projected_1 = A_mean @ V_T_1.T\n",
        "\n",
        "print(\"\\nSuppose we project our data onto the first principal component, i.e. setting k = 1:\\n\", A_projected_1)\n",
        "\n",
        "A_approx_1 = A_projected_1 @ V_T_1 + np.mean(A, axis=0)\n",
        "\n",
        "print(\"\\nWe can take our projection and multiply it by the first column of V_T to approximate A:\\n\", A_approx_1)\n",
        "\n",
        "V_T_2 = V_T[:2, :]\n",
        "A_projected_2 = A_mean @ V_T_2.T\n",
        "\n",
        "print(\"\\nNow, let's observe the difference when we set k = 2, i.e. projecting our matrix onto the first two principal components:\\n\", A_projected_2)\n",
        "\n",
        "A_approx_2 = A_projected_2 @ V_T_2 + np.mean(A, axis=0)\n",
        "\n",
        "print(\"\\nLet's use this to approximate A with k=2:\\n\", A_approx_2)\n",
        "\n",
        "V_T_3 = V_T[:3, :]\n",
        "A_projected_3 = A_mean @ V_T_3.T\n",
        "\n",
        "print(\"\\nFinally, let's project the matrix onto all three principal components, i.e. setting k = 3:\\n\", A_projected_3)\n",
        "\n",
        "A_approx_3 = A_projected_3 @ V_T_3 + np.mean(A, axis=0)\n",
        "\n",
        "print(\"\\nSince we used the same number of principal components as our original matrix, we yield an exact reconstruction of A:\\n\", A_approx_3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAdurI5XOCZ0",
        "outputId": "906c4f7a-f039-4dc0-f0d3-678ad4c39dd7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter 6 numbers, separating by commas (i.e. 1,2,3,4,5,6): 12,125,894,89,8,7\n",
            "\n",
            "Let A be a 2x3 matrix with entries in R:\n",
            "[[ 12. 125. 894.]\n",
            " [ 89.   8.   7.]]\n",
            "\n",
            "First, we need to center the data by subtracting the mean of each column.\n",
            "This will simplify the calculations we have to do later and remove bias from columns with large magnitudes:\n",
            " [[ -38.5   58.5  443.5]\n",
            " [  38.5  -58.5 -443.5]]\n",
            "\n",
            "We'll use singular value decomposition on A to find matrices U, S, and V^T.\n",
            "\n",
            "U:\n",
            " [[-0.70710678  0.70710678]\n",
            " [ 0.70710678  0.70710678]]\n",
            "\n",
            "S (Singular values):\n",
            " [6.34975196e+02 4.01943669e-14]\n",
            "\n",
            "V^T (our principal components):\n",
            " [[ 0.08574701 -0.13029091 -0.98776097]\n",
            " [-0.98776097  0.11853247 -0.10138207]\n",
            " [ 0.13029091  0.98436494 -0.11853247]]\n",
            "\n",
            "Suppose we project our data onto the first principal component, i.e. setting k = 1:\n",
            " [[-448.99526724]\n",
            " [ 448.99526724]]\n",
            "\n",
            "We can take our projection and multiply it by the first column of V_T to approximate A:\n",
            " [[ 12. 125. 894.]\n",
            " [ 89.   8.   7.]]\n",
            "\n",
            "Now, let's observe the difference when we set k = 2, i.e. projecting our matrix onto the first two principal components:\n",
            " [[-4.48995267e+02 -1.04152797e-14]\n",
            " [ 4.48995267e+02  1.04152797e-14]]\n",
            "\n",
            "Let's use this to approximate A with k=2:\n",
            " [[ 12. 125. 894.]\n",
            " [ 89.   8.   7.]]\n",
            "\n",
            "Finally, let's project the matrix onto all three principal components, i.e. setting k = 3:\n",
            " [[-4.48995267e+02 -1.04152797e-14 -2.62984079e-15]\n",
            " [ 4.48995267e+02  1.04152797e-14  2.62984079e-15]]\n",
            "\n",
            "Since we used the same number of principal components as our original matrix, we yield an exact reconstruction of A:\n",
            " [[ 12. 125. 894.]\n",
            " [ 89.   8.   7.]]\n"
          ]
        }
      ]
    }
  ]
}